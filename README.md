SQL Alchemy

Surfs Up - Now that the foundation for SQL and python has been laid, the ability to use Python to extract and manipulate Data from a SQL query is the next goal. Using the declarative_base, which allows the user to define/create schema, the user can then create Classes(Tables) which are built upon the Base. In this case, the goal is to create two separate classes, Surfers and Boards. The name of the table and each Column must be indiviudal instatiated. By creating a connection to a database with create_engine,the path and file name have been instatiated. By using Base.metadata.create_all, there is now a SQLite DB with the created Table with Columns. The next objective is to create instances for both classes. Once a surfer and board instance are created, the next step is to instatiate a session, add the classes, and finally commit the changes. Lastly, the newly created database can be accessed through Python using session.query.

Emoji Plotting -  After importing the new dependencies, the first step is to create an engine which connects with SQL database, where the data is located.The user can either go directly to the SQL DB, but if that is not possible, it it important to extract both table and column information before running queries. This is accomplished through the .get_table_names() and .get_columns('table_name') functions. Once we have the table and column and table info, loop through the columns variable and print the information needed, which in this case is 'name' and 'type'. Once exploration and analysis is complete. There are several routes one can take to extract and explore the information to be analyzed. In this instance, we want to create a reflection, which mirrors the structure of the existing table into our metadata. Here, automap_base() automaticall defines the structure of the class. The next step is to instatiate a session with the DB. Once the session is established, it is now possible to query the table for the columns needed for analysis. The emoji character, ID and score are all extracted but are served up in a Tuple. The data is unpacked and stored into separate list using list comprehensions. The separate lists are then plotted in MatPlotLib & turned into a DataFrame and plotted in Pandas. THe final goal is to use pd.read_sql to connect to our engine and access the Table through a more streamlined process.

Hawaii Weather - 
